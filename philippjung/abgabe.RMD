---
title: "Abgabe"
output:
  pdf_document: default
---
# Minimum Variance Portfolio

## Data
For portfolio construction, two different datasets are considered. The first one being 'Ten sector portfolios of the S&P 500 and the US equity market portfolio' by Roberto Wessels. Note that for this dataset, it is necessary to substract the Treasury Bill-rates from each entry to obtain adjusted returns stripped of the risk-free rate.

The other dataset concerned is provided by Kenneth French and downloaded from his website. The dataset is called 'SMB and HML portfolios and the US equity market portfolio' and contains the factors "Small Minus Big" (SMB) and "High Minus Low" (HMB) from the Fama-French three-factor model as well as the whole S&P500 portfolio. This dataset is already cleared from the risk-free rate, thus a Treasure Bill-rate adjustment is not necessary.

![Explanation 10 Industry Portfolios](10-industry-portfolios-explanation.png)

### Loading the data
Read 'average value weighted returns (monthly)' from line 12 to line 1121
```{r 10-industry-portfolios}
return_matrix = read.csv('../ten-roberto-wessels.csv', header=TRUE, sep=',')
colnames(return_matrix)[1] = "Date"
experiment = subset(return_matrix, select = -c(13))
experiment = apply(experiment[,-1], 2, '-', return_matrix[,13])
return_matrix = cbind(return_matrix$Date, experiment)
colnames(return_matrix)[1] = 'Date'
return_matrix
```

## Computing the Covariance Matrix
As we saw in the section above, to minimize the overall variance, we first need to compute the covariance matrix. This can be achieved by applying the cov() function to the rows (t-th row for time t) of return_matrix.
```{r}
benchmark_matrix = data.matrix(return_matrix)
nA = dim(benchmark_matrix[,-1])[2] # number of assets
cov_benchmark_matrix = cov(benchmark_matrix[,-1]) # dates are deleted
```
All positive values in the cov_return_matrix, meaning there is at least some linear relation between all the values. 

By evaluating the restrictions and definitions of the minimization-problem, you can define a Lagrangian, which can later be transposed into a linear system of equations of form
$$A \cdot x = b.$$ 
Since $A = \Sigma$ and $b = 1$ are known, we can solve this system to obtain $x = w_t.$

```{r}
b = vector(length=nA) + 1
weights = solve(cov_benchmark_matrix, b)
weights = weights/abs(sum(weights))
mean_returns = data.matrix(apply(benchmark_matrix[,-1], 2, mean))
weights = data.matrix(weights)
sharpe_ratio_IS = t(mean_returns) %*% weights / sqrt(t(weights)%*%cov_benchmark_matrix%*%weights)
sharpe_ratio_IS
```
Note that negative weights are interpreted as *short sales*.

## Performance anaylsis
To evaluate the performance, the analysis from deMiguel will be mimiced. DeMiguel et al. used a rolling-sample approach, where over a time period $T = 120$months, returns of the assets were being calculated. In each month $t$, starting form $t_0 = M + 1$, the data from the previous $M$ months was used to calculate the portfolio-weights $w_t$.
By adding the return of the next period and dropping the earliest return, the whole dataset is processed, resulting in a series of $T - M$ monthly *out-of-sample* returns generated by each strategy.

Then, the *out-of-sample Sharpe ratio* is computed. It consists of the sample mean of out-of-sample excess return $\mu_k$, devided by their sample standard deviation $\sigma_k$:
$$SR = \dfrac{\mu_k}{\sigma_k}$$

```{r}
nRows = dim(return_matrix)[1] # no of months
M = 120 # size of rolling-sample
t = M + 1 # adjust t_0 to size of rolling-sample
mu = c() # empty vector of excess-return
while (t <= nRows-1) {
  tStart = t - M # lower barrier for the time-interval
  tEnd = t # upper barrier for the time-interval
  return_matrix_t = return_matrix[c(tStart:tEnd),] # return matrix for time-interval
  return_matrix_future = return_matrix[c(tEnd+1),] # return matrix one month in the future
  cov_return_matrix_t = cov(data.matrix(return_matrix_t[,-1])) # calculate cov-matrix
  b_t = vector(length=nA) + 1 # set up system of equations
  x_t = solve(cov_return_matrix_t, b_t) # and solve it
  x_t = x_t/sum(x_t) # normalize weights
  if(t == M+1) {
    historyOfWeights = data.frame(t(x_t)) # create df
  }
  else {
    historyOfWeights = rbind(historyOfWeights, x_t) # save weights
  }
  # get out-of-sample excess return for time T
  mu = c(mu, x_t %*% as.numeric(return_matrix_future[1,-1]))
  t = t+1
}
head(mu)
mean(mu)
View(historyOfWeights)
```


## Plotting
Alla proposed differend kinds of plotting to us, two of them being a portfolio appreciation graph and the dynamics of weights. 

### portfolio appreciation graph
```{r}
library('zoo')
library('xts')
historyOfReturns = cumsum(mu) + 100
View(historyOfReturns)

toXts = data.frame(return_matrix[122:length(return_matrix[,1]),1], historyOfReturns)
colnames(toXts) = c('Date', 'Out-of-sample returns')
toXts$Date = as.yearmon(as.character(toXts$Date), format="%Y%m")
portfolio_appreciation = as.xts(toXts$`Out-of-sample returns`, order.by=toXts$Date)
plot(portfolio_appreciation, type='l') 
```

### Dynamics of weights
```{r}
weightsHistoryDf = data.frame(return_matrix[122:dim(return_matrix)[1],1], historyOfWeights)
View(weightsHistoryDf)
colnames(weightsHistoryDf)[1] = "Date"
weightsHistoryDf$Date = as.yearmon(as.character(weightsHistoryDf$Date), format="%Y%m")
weightsHistoryXts = as.xts(weightsHistoryDf[,-1], order.by=weightsHistoryDf$Date)
plot(weightsHistoryXts, type='l')
```